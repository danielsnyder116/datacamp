{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks account for interactions well (better than linear regressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input layer - predictive features\n",
    "- output layer - outcome\n",
    "- hidden layers - nothing we observe directly - black box\n",
    "- each node represents an aggregation of data from input layer\n",
    "- the more nodes, the more interactions we captuire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward propagation - an algorithm that moves from inputs on the left to hidden layer and then to the output\n",
    "- multiply, add process - dot product\n",
    "\n",
    "- When we are fitting our model, they change the weights\n",
    "-  In general, for one data point at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 1]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#Forward Propagation Code\n",
    "\n",
    "input_data = np.array([2,3])\n",
    "\n",
    "weights = {'node_0':np.array([1,1]),\n",
    "           'node_1':np.array([-1, 1]),\n",
    "           'output':np.array([2,-1])}\n",
    "\n",
    "node_0_value = (input_data * weights['node_0']).sum()\n",
    "node_1_value = (input_data * weights['node_1']).sum()\n",
    "\n",
    "\n",
    "hidden_layer_values  = np.array([node_0_value, node_1_value])\n",
    "print(hidden_layer_values)\n",
    "\n",
    "output = (hidden_layer_values * weights['output']).sum()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Activation Function ( in hidden layers) - allows function to capture non-l-nearities \n",
    "-  Necessary for relations that are not linear (not straight lines )\n",
    "- applied to node inputs to produce node output\n",
    "\n",
    "- tanh - used to be most popular\n",
    "- today, industry standard is  \"relu - rectified linear activation function\" = rectified linear unit (ReLU)\n",
    "\n",
    " RELU(x) = {0 if x <0,\n",
    "            x if x>=0}\n",
    "\n",
    "- Identity function - a node's output will be the same as its input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999092  0.76159416]\n",
      "1.2382242525694254\n"
     ]
    }
   ],
   "source": [
    "#Forward Propagation Code WITH Activation Function included\n",
    "\n",
    "input_data = np.array([2,3])\n",
    "\n",
    "weights = {'node_0':np.array([1,1]),\n",
    "           'node_1':np.array([-1, 1]),\n",
    "           'output':np.array([2,-1])}\n",
    "\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "\n",
    "node_0_output = np.tanh(node_0_input)\n",
    "node_1_output = np.tanh(node_1_input)\n",
    "\n",
    "\n",
    "hidden_layer_outputs  = np.array([node_0_output, node_1_output])\n",
    "print(hidden_layer_outputs)\n",
    "\n",
    "output = (hidden_layer_outputs * weights['output']).sum()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural networks partially replace the need for feature engineering\n",
    "\n",
    "- Deep Learning = Representation Learning\n",
    "- Modeler doesn't need to specify interactions\n",
    "- When you train the model, the neural network gets weights that find the relevant patterns\n",
    "    to make better predictions\n",
    "    \n",
    "- Error = Predicted - Actual/Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Loss Function - function that aggregates errors in prediction from many data points into a single number\n",
    "\n",
    "- For Example: Loss function for Linear Regression: Mean Squared Error (MSE)\n",
    "- You take the error for each prediction, square it, and then take the mean of all of them \n",
    "        \n",
    "        \n",
    " ### To find the weights that give the lowest value for the loss function, we use:\n",
    "-     Gradient Descent:\n",
    "-         - start at a random point\n",
    "-         - until the slope is flat (equal to zero), find the slope and take a step down\n",
    "-         - rather than using the amount of error, we use a:\n",
    "-         learning rate: updates each weight by subtracting learning rate * slope\n",
    "-            - frequently around 0.01 - ensures we take small steps to reliably move to minimum of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([30, 40])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code to calculate the slopes and then update weights\n",
    "\n",
    "weights = np.array([1,2])\n",
    "input_data = np.array([3,4])\n",
    "target = 6\n",
    "learning_rate = .01\n",
    "\n",
    "preds = (weights * input_data).sum()\n",
    "error = preds - target\n",
    "\n",
    "print(error)\n",
    "\n",
    "gradient = 2 * input_data * error\n",
    "gradient\n",
    "\n",
    "#Gradient (similar to derivative, but only for a specific point, not the whole function)\n",
    "                            #This is called \"error\"\n",
    "#Gradient for MSE is 2* (predicted - actual) - which makes sense since derivative of f(x) = x^2 is f'(x)=2x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_updated = weights - learning_rate * gradient\n",
    "\n",
    "preds_updated = (weights_updated * input_data).sum()\n",
    "\n",
    "error_updated = preds_updated - target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tWeights_Updated:[0.7 1.6],\n",
      "        Preds_Updated:\t8.5,\n",
      "        Error_Updated:\t2.5\n",
      "       \n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\\tWeights_Updated:{weights_updated},\n",
    "        Preds_Updated:\\t{preds_updated},\n",
    "        Error_Updated:\\t{error_updated}\n",
    "       \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation - takes error from output layer and propagates backwards towards input layer\n",
    "-    allows gradient descent to update all weights in the neural network (by getting gradients for all of the weights)\n",
    "-    comes from chain rule in calculus\n",
    "\n",
    "- We always do forward propagation before backpropagation\n",
    "\n",
    "### Known as Stochastic Gradient Descent\n",
    "- It is common to calculate the slopes on only a subset of the data - called a \"Batch\"\n",
    "- Uses different stochastic (greek for to aim/guess -from Jacob Bernoulli book - Russian dude) or random batch\n",
    "    -to calculate the next update\n",
    "    -Once all the data has been used, we start again from the beginning\n",
    "    -Each time through all the training data is called and EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KERAS Model Building Steps\n",
    "1. Specify Architecture - how many layers, how many nodes, activation function\n",
    "2. Compile - specifies loss function, and details on optimization\n",
    "3. Fit the model - Cycle of backpropagation and optimization of weights\n",
    "4. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0. Getting all the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', None)\n",
    "\n",
    "#Loading data\n",
    "predictors = pd.read_csv('wages.csv')\n",
    "\n",
    "#Specifies how many columns are in input -> number of nodes in input layer\n",
    "n_cols = predictors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage_per_hour</th>\n",
       "      <th>union</th>\n",
       "      <th>education_yrs</th>\n",
       "      <th>experience_yrs</th>\n",
       "      <th>age</th>\n",
       "      <th>female</th>\n",
       "      <th>marr</th>\n",
       "      <th>south</th>\n",
       "      <th>manufacturing</th>\n",
       "      <th>construction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.10</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.95</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.67</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.50</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wage_per_hour  union  education_yrs  experience_yrs  age  female  marr  \\\n",
       "0           5.10      0              8              21   35       1     1   \n",
       "1           4.95      0              9              42   57       1     1   \n",
       "2           6.67      0             12               1   19       0     0   \n",
       "3           4.00      0             12               4   22       0     0   \n",
       "4           7.50      0             12              17   35       0     1   \n",
       "\n",
       "   south  manufacturing  construction  \n",
       "0      0              1             0  \n",
       "1      0              1             0  \n",
       "2      0              1             0  \n",
       "3      0              0             0  \n",
       "4      0              0             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\602770\\AppData\\Local\\Continuum\\anaconda3\\envs\\biking\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#1. Model Specification - this is the easier way\n",
    "#Sequential requires that each layer has weights/connections only to the one layer coming directly after it\n",
    "model = Sequential()\n",
    "\n",
    "#We add layers - standard layer is called Dense - all nodes in previous layer connect to all nodes in current layer\n",
    "\n",
    "#Each layer, specify number of nodes, and activation function\n",
    "#Input will have n_cols columns, and nothing after column, which indicates that we can have any number of rows/data points\n",
    "model.add(Dense(100, activation='relu', input_shape= (n_cols,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "#Output layer has one node - \n",
    "model.add(Dense(1,))\n",
    "\n",
    "#common to use 100s or 1000s of nodes in a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Compiling - two arguments, optimizer & loss function\n",
    "#Adam is a good choice - adjusts lr as it changes grad. descent\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Fit the model - apply backpropagation and gradient descent to update weights\n",
    "#One can improve process by scaling data so each feature is on average a simialr size value\n",
    "#Common approach: data point - mean divided by standard deviation\n",
    "\n",
    "#wont actually work unless target is in form of a numpy matrix \n",
    "#model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When completing a CLASSIFICATION prediction vs Regression:\n",
    "- Set the loss function equal to \"categorical_crossentropy\" - most common - similar to log loss but not the same\n",
    "- Lower score is better\n",
    "- adding argument metrics= ['accuracy'] helps provide easy-to-understand diagnostics\n",
    "- Output layer needs to have a separate node for each possible outcome and uses \"softmax\" activation\n",
    "    - softmax ensures that the predictions sum to 1 so they can be interpreted as probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\602770\\AppData\\Local\\Continuum\\anaconda3\\envs\\biking\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\602770\\AppData\\Local\\Continuum\\anaconda3\\envs\\biking\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "534/534 [==============================] - ETA: 2s - loss: 6.8621 - accuracy: 0.0000e+ - 0s 337us/step - loss: 4.0046 - accuracy: 0.0618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2207531ffd0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CODING EXAMPLE\n",
    "\n",
    "#One Hot Encoding converts result values into individual columns\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "data = pd.read_csv('wages.csv')\n",
    "\n",
    "\n",
    "predictors = data.drop(['wage_per_hour'], axis=1).as_matrix()\n",
    "n_cols = predictors.shape[1]\n",
    "target = to_categorical(data['wage_per_hour'])\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "#Number of output nodes is the number of classification categories\n",
    "model.add(Dense(45, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  8, 21, ...,  0,  1,  0],\n",
       "       [ 0,  9, 42, ...,  0,  1,  0],\n",
       "       [ 0, 12,  1, ...,  0,  1,  0],\n",
       "       ...,\n",
       "       [ 1, 17, 25, ...,  0,  0,  0],\n",
       "       [ 1, 12, 13, ...,  1,  0,  0],\n",
       "       [ 0, 16, 33, ...,  0,  1,  0]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Be able to SAVE, RELOAD, and PREDICT with your model\n",
    "\n",
    "#SAVING Model\n",
    "from keras.models import load_model\n",
    "\n",
    "#Hdf 5 is the format of the model\n",
    "\n",
    "model.save('first_model.h5')\n",
    "\n",
    "my_model = load_model('first_model.h5')\n",
    "\n",
    "predictions = my_model.predict(data_to_predict_with)\n",
    "\n",
    "#Gets the second column of the probability of the shot being made - True\n",
    "probability_true = predictions[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIMIZATION IS HARD\n",
    "SGD - Stochastic Gradient Descent is the safest optimizer\n",
    "\n",
    "#Learning rate can be too low or too high\n",
    "#can specify learning rate \n",
    "my_optimizer = SGD(lr=lr)\n",
    "\n",
    "#Dying Neuron Problem\n",
    "Node takes value of less than zero for all rows of your data\n",
    "Once a node starts always getting negative inputs, it may continue to \n",
    "    only get negative inputs - thus it contributes nothing to the\n",
    "    model and is \"dead\"\n",
    "    \n",
    "But the solution is not to change the activation function from \"relu\" which makes\n",
    "    any negative value (and its slope) zero\n",
    "    \n",
    "For many years, used to use the tanh function\n",
    "But in deep learning, multiplying many slopes of small values led them to move\n",
    "    towards zero, known as vanishing gradients problem \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL VALIDATION - for deep learning, people rarely do k-fold validation - takes too long\n",
    "model.fit(predictors, target, validation_split=.3) #specifies what fraction of data is used for validation\n",
    "\n",
    "#We should stop training when validation score is not improving\n",
    "#We can use Early Stopping to do this\n",
    "#We then create this setup before we fit our model\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=2) #how many epochs model can go without improving before stopping \n",
    "\n",
    "#Normally more than 3 epochs is unlikely to improve\n",
    "\n",
    "#By default, keras trains for 10 epochs\n",
    "model.fit(predictors, target, validation_split=.3, epochs=20, verbose=False #prints out fewer updates\n",
    "         callbacks=[early_stopping_monitor]) #callbacks takes a list - can add other callbacks for more advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation score is ultimate measure of model's predictive quality\n",
    "\n",
    "\n",
    "#MODEL CAPACITY / network Capacity - similar to overfitting and underfitting\n",
    "\n",
    "\n",
    "#Ability to capture predictive patterns in data - more capacity, further to right on bias variance graph\n",
    "#adding neurons or layers - moves further to right (higher complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKFLOW:\n",
    "    1.Start with a small network\n",
    "    2.Get the validation score\n",
    "    3.Keep increasing capacity until validation score is no longer improving\n",
    "    4.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
